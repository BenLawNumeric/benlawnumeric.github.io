<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="ex2.css" />
    <title>Rapport de la conférence Maël Pégny</title>
</head>
<body>
    <h1> Rapport sur l'Étude et perspectives de la conférence sur l'intelligence artificielle et la vie privée </h1>
    <div class="introduction">
        <p> Depuis 2017, la CNIL appelle à la vigilance concernant les évolutions des outils de vidéoprotection. En effet, la multiplication de certains dispositifs de vidéo « augmentée » pose des questions éthiques et juridiques nouvelles. Aujourd’hui, afin d’accompagner leur déploiement dans le respect des droits des personnes, la CNIL soumet un projet de proposition à consultation publique jusqu’au 11 mars 2022. De tels dispositifs ne sont en aucun cas un simple « prolongement » technique des caméras existantes. Ils modifient leur nature même par leur capacité de détection et d’analyse automatisée.</p>
        <p> L’intelligence artificielle (IA) désigne tout système capable d’accomplir des tâches d’une manière que nous percevons comme “intelligente”. Autrement dit, il s’agit de l’ensemble de techniques, théories et sciences qui simulent les capacités cognitives de l’être humain . Ainsi, l’abstraction, la créativité, la déduction, la résolution de problèmes, la prise de décision ou la capacité d’apprendre, sont associées à l’IA. Une différence doit notamment être faite entre IA dite “forte” et IA dite « faible ». L’IA « forte » serait en capacité de contextualiser des problèmes spécialisés très différents de manière totalement autonome.</p>
        <p> Toutefois, selon le Conseil de l’Europe, aucune technologie connue ne permet de démontrer l’existence actuelle d’IA « forte ». Par opposition, l’IA « faible » ou “modérée” n’est en capacité d’être performante que dans son domaine d'entraînement. Quant au Machine Learning, il représente un ensemble de méthodes puissantes qui permettent de créer des modèles prédictifs à partir de données sans avoir été explicitement programmés. Ce type d’intelligence artificielle est mis en cause aujourd’hui. </p>
        <p> Se pose alors la question des conséquences d’une généralisation hâtive de l’Intelligence artificielle. Le chercheur postdoctoral Pégny Maël , lors de deux matinées d’études organisées par l’Université Paris-Est Créteil, a évoqué les problématiques pouvant exister entre droit au respect de la vie privée et développement des IAs. En effet, le développement de l’Intelligence artificielle a apporté de nombreuses incertitudes quant à son développement éthique en raison du renforcement des inégalités et de l’atteinte à la vie privée que pourrait engendrer son emploi dans différents secteurs tels que notamment la santé, l’assurance ou les services publiques. </p>
        <p> Face aux différents enjeux exposés et à la nécessité d’identifier les différents enjeux et ordres juridiques présents en la matière, il est aujourd’hui essentiel de traiter de la notion d’IA et de vie privée. </p>
        <p> Dans ce cadre, une exposition liminaire du point de vue de Maël Pégny (I) paraît essentielle afin de fixer le cadre de réflexion. Ainsi, ce cadre permettra dans un second temps d’identifier la nécessité d’une analyse poussée face à l’ère du développement de l’IA (II) afin de les envisager à l’aune des propositions actuelles (III).</p>
    </div>

    <div class="partie1">
        <h2> <u> I. Une appréhension globalisée du droit au respect de la vie privée et développement des IAs. </u> </h2>
        <p> Au sein de sa restitution et après avoir exposé le fait que la vie privée est une donnée entretenant des liens importants avec les institutions étatiques, Maël Pégny évoque les enjeux de la collecte de données personnelles, à savoir les questions relatives à la collecte et la formation de connaissances (réelles ou prétendues) sur les individus. L’explosion de la collecte induit l’automatisation de l’analyse. On définit l’intelligence artificielle par son objectif : créer des systèmes informatiques intelligents en exécutant des tâches communément considérées comme intelligentes. </p>
        <p> Une fois ces définitions posées, Maël Pégny présente les problématiques relatives à « L’âge d’or de la surveillance ” : la surveillance de masse en passant par le capitalisme de la surveillance pour finir par la propagande de masse et le profilage des individus. Enfin ont été abordées les notions de mort de l’anonymat en ligne et les enjeux qu’elle soulève. </p>
        <p> S’appliquent à l’IA des prises de position fortes pour interdire certaines pratiques économiques manipulatoires, exploitatrices ou visant au contrôle social. L’IA fait également face à une exigence de transparence, en informant sur l’existence d’une interaction avec un robot ou sur la détection d’émotion. Maël Pégny soulève la question suivante : pourquoi ces restrictions s'appliquent- elles seulement à des systèmes qualifiés d’IA ? </p>
        <p> La reconnaissance faciale est l’exemple type d’intelligence artificielle. Elle regroupe deux formes d’outils : la reconnaissance à partir de photographies et la reconnaissance en direct à partir d’images de caméras de vidéo de surveillance dans l’espace public (FRT). C’est cette seconde forme qui focalise le débat. En effet, elle marque une rupture juridique en appliquant à l’ensemble de la population des pratiques d’identification réservées aux suspects et criminels avérés, et une rupture symbolique : c’est la fin de l’anonymat dans l’espace public. 
            Ainsi, il ne faut pas surestimer l’efficacité de la FRT, qui connaît énormément de dysfonctionnements avec un taux de réussite de 0% sur des tests menés en 2016. Cependant, cela pose un problème de fond majeur : que se passerait-il si la technique fonctionnait bien ? Deux écoles de pensée s’opposent. D’une part, l’école réglementaire qui défend un usage très limité de la reconnaissance faciale dans les cas de terrorisme ou de disparition d’enfant par exemple. D’autre part, l’école prohibitionniste qui défend le fait qu’un tel dispositif ne devrait pas exister. 
            En outre, le développement de ce type d’IA pose deux questions essentielles : la question du développement de la technologie et notamment des bases de données d’entraînement qui doivent être créées, et la question de l’usage de cette technologie et surtout du contrôle institutionnel. </p>
        <p> On se rend compte de la nécessité de la création de vastes bases de données collectées dans des conditions naturelles pour l'entraînement des modèles. L’approche d’usage réglementé entérine la collecte de données à l’échelle d’une population sans consentement. Maël Pégny alerte quant à la perte de sensibilité contemporaine face à cette collecte et se pose la question de la fidélité à l’esprit originel du droit des données personnelles. </p>
        <p> Le développement de la FRT constitue en soi un tournant juridique et le problème du contrôle institutionnel est une question cruciale. Est-on capable de borner l’emploi d’un outil de contrôle social ? La question peut raisonnablement être soulevée au regard de certaines pratiques inquiétantes dans les polices de certains pays démocratiques, notamment aux Etats-Unis qui sont source d’inquiétude comme les identifications de manifestants “Black Lives Matter'', ou le programme FBI “COINTELPRO” dont l’objectif fut d'enquêter sur les organisations politiques dissidentes aux Etats-Unis et de perturber leurs activités. Dans la Résolution du Parlement européen du 20 octobre 2020 concernant un cadre pour les aspects éthiques de l’intelligence artificielle, de la robotique et des technologies connexes (2020/2012(INL)), est interdite l’identification biométrique à distance dans l’espace public en temps réel à des fins répressives. Cependant, au regard du nombre d’exceptions, cela ressemble davantage à une autorisation plutôt qu'à une prohibition puisque sont listées toutes les infractions de droit commun.</p>
        <p> Toutes ces questions ont un impact conceptuel sur notre ordre légal et notamment sur les données personnelles. C’est une notion extrêmement large et sémantique. L’IA a pour conséquence de déduire des choses des données qu’on lui transmet : “Vous ne pouvez protéger ce que je peux déduire”. Ce n’est pas la donnée qu’il faut protéger, mais la connaissance qu’elle contient. Dans ce cadre, Maël Pégny alerte sur la nécessité d’une régulation de l’inférence statistique  et notamment sur la validité scientifique des inférences (droit à l’inférence raisonnable) et de la protection de la vie privée contre le contournement statistique. Aujourd’hui, il est possible de procéder à des inférences d’appartenance (déduction de l’appartenance d’un sujet à la base d’entrainement). </p>
        <p> Cependant, le problème prépondérant est que le droit des données de l’UE est centré sur les données avant traitement. Une donnée est personnelle si elle permet d’identifier une personne physique. Il y a une nécessité de clarifier le rapport entre traitement et statut de donnée personnelle. Ces problématiques sont mises en lumière par le Groupe de travail “article 29” , sur la protection des données. Trois critères des données personnelles peuvent être retenus : contenu, intention, résultat. Avec l’exploitation des données environnementales pour influencer les personnes dans les smart cities , tout peut devenir une donnée personnelle par intention ou par résultat. </p>
        <p> Enfin, Maël Pégny nous rappelle qu’il ne faut pas penser qu’en termes de barrières mais il s’agit plutôt de se poser la question suivante : comment réorienter l’appareil de connaissance numérique des individus vers des fins saines ?  Quelle reconnaissance par les institutions les individus souhaitent-ils ? </p>
        <p> En effet, la reconnaissance des individus par les institutions peut être désirée. Ce fut le cas notamment des femmes amérindiennes qui habitaient sur les réserves. Une base centralisée est la première chose qui a été demandée pour leur permettre de jouir des mêmes droits que les femmes américaines, notamment en ce qui concerne les violences sexuelles. </p>
    </div>

    <div class="partie2">
        <h2> 
        <p> 
            
        </p>
        
    </div>
</body>
</html>